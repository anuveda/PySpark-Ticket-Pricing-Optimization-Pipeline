# -*- coding: utf-8 -*-
"""FanEngagementSeatGeek (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JOe5QOECB5dcf42ptJM_5mybZ4s4GsMl
"""

pip install pyspark

pip install pandas matplotlib seaborn

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, datediff, current_date, hour, dayofweek
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Ticket Price Prediction") \
    .getOrCreate()

# Load the data
file_path = "/content/combined_lafc_nashville_data_frame.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Display the schema
df.printSchema()

# Basic preprocessing: handling missing values, casting columns to correct types
df = df.na.drop()
df = df.withColumn("deal_score", col("deal_score").cast("double"))
df = df.withColumn("section", col("section").cast("double"))
df = df.withColumn("total_ticket_price", col("total_ticket_price").cast("double"))

# Feature Engineering
df = df.withColumn("days_until_event", datediff(col("datetime_utc"), current_date()))
df = df.withColumn("event_hour", hour(col("datetime_local")))
df = df.withColumn("event_dayofweek", dayofweek(col("datetime_local")))

# Select relevant features
features = [
    "days_until_event", "event_hour", "event_dayofweek", "deal_score",
    "quantity", "fee", "section"
]

# Assemble features into a feature vector
assembler = VectorAssembler(inputCols=features, outputCol="features")
data = assembler.transform(df)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, datediff, current_date, hour, dayofweek, when
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Ticket Price Prediction") \
    .getOrCreate()

# Load the data
file_path = "/content/combined_lafc_nashville_data_frame.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Display the schema
df.printSchema()

# Basic preprocessing: handling missing values, casting columns to correct types
df = df.na.drop()
df = df.withColumn("deal_score", col("deal_score").cast("double"))
df = df.withColumn("section", col("section").cast("double"))
df = df.withColumn("total_ticket_price", col("total_ticket_price").cast("double"))

# Verify data types and check for null values
df.select([col for col in df.columns]).show()

# Replace null values in features with appropriate values
df = df.fillna({'deal_score': 0, 'section': 0, 'quantity': 0, 'fee': 0})

# Feature Engineering
df = df.withColumn("days_until_event", datediff(col("datetime_utc"), current_date()))
df = df.withColumn("event_hour", hour(col("datetime_local")))
df = df.withColumn("event_dayofweek", dayofweek(col("datetime_local")))

# Select relevant features
features = [
    "days_until_event", "event_hour", "event_dayofweek", "deal_score",
    "quantity", "fee", "section"
]

# Assemble features into a feature vector
assembler = VectorAssembler(inputCols=features, outputCol="features")
data = assembler.transform(df)

# Split the data into training and testing datasets
train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)

# Initialize the model
rf = RandomForestRegressor(featuresCol="features", labelCol="total_ticket_price")

# Build the parameter grid for hyperparameter tuning
paramGrid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [20, 50]) \
    .addGrid(rf.maxDepth, [5, 10]) \
    .build()

# Initialize cross-validator
crossval = CrossValidator(estimator=rf,
                          estimatorParamMaps=paramGrid,
                          evaluator=RegressionEvaluator(labelCol="total_ticket_price"),
                          numFolds=3)

# Train the model
cvModel = crossval.fit(train_data)

# Make predictions on the test data
predicted_total_ticket_price = cvModel.transform(test_data)

# Evaluate the model
evaluator = RegressionEvaluator(labelCol="total_ticket_price")
rmse = evaluator.evaluate(predicted_total_ticket_price, {evaluator.metricName: "rmse"})
mae = evaluator.evaluate(predicted_total_ticket_price, {evaluator.metricName: "mae"})
r2 = evaluator.evaluate(predicted_total_ticket_price, {evaluator.metricName: "r2"})

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared: {r2}")

predicted_total_ticket_price.select("event_id", "title", "datetime_utc", "total_ticket_price", "prediction", "deal_score").show(500)

# Check the range of dates in the dataset
df.selectExpr("min(datetime_utc) as start_date", "max(datetime_utc) as end_date").show()

from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import StandardScaler

# Select relevant features for clustering
features = ["ticket_price", "deal_score", "quantity", "fee", "section"]
assembler = VectorAssembler(inputCols=features, outputCol="features")
data = assembler.transform(df)

# Scale the features
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
data = scaler.fit(data).transform(data)

# Apply KMeans clustering
kmeans = KMeans(k=5, seed=1, featuresCol="scaled_features")
model = kmeans.fit(data)

# Make predictions
predictions = model.transform(data)
predictions.select("event_id", "title", "prediction").show(500)

# Example of generating targeted marketing campaign
high_engagement_fans = predictions.filter(predictions.prediction == 1)
high_engagement_fans.select("event_id", "title", "total_ticket_price").show(500)

# Example of providing real-time seat upgrades
real_time_upgrades = df.withColumn("upgrade_offer", when(df["deal_score"] > 7, "Premium Seat Upgrade").otherwise("Standard Offer"))
real_time_upgrades.select("event_id", "title", "deal_score", "upgrade_offer").show(2500)

#visualization
import matplotlib.pyplot as plt

# Extract cluster centers
centers = model.clusterCenters()
centers = [center[:2] for center in centers]

# Extract cluster assignments
assignments = model.transform(data).select("prediction").rdd.flatMap(lambda x: x).collect()

# Extract features for visualization
features = data.select("ticket_price", "deal_score").collect()
features = [(f[0], f[1]) for f in features]

# Plot the clusters
plt.figure(figsize=(10, 6))
for i in range(len(features)):
    plt.scatter(features[i][0], features[i][1], c=assignments[i], cmap="viridis", s=50)
plt.scatter([center[0] for center in centers], [center[1] for center in centers], c="red", s=200, marker="x")
plt.xlabel("Ticket Price")
plt.ylabel("Deal Score")
plt.title("KMeans Clustering")
plt.show()

from pyspark.sql.functions import abs, col

# Select relevant columns from the predictions
comparison = predicted_total_ticket_price.select(
    "event_id",
    "title",
    "datetime_utc",
    "total_ticket_price",
    col("prediction").alias("predicted_total_ticket_price"),
    "deal_score"
)

# Calculate the absolute error
comparison = comparison.withColumn(
    "absolute_error", abs(col("total_ticket_price") - col("predicted_total_ticket_price"))
)

# Show the comparison
comparison.show(20)

# Calculate summary statistics to evaluate the model performance
summary_stats = comparison.selectExpr(
    "percentile(absolute_error, 0.5) as median_absolute_error",
    "percentile(absolute_error, 0.75) as upper_quartile_absolute_error",
    "percentile(absolute_error, 0.25) as lower_quartile_absolute_error",
    "avg(absolute_error) as mean_absolute_error"
)

summary_stats.show()

# Visualize the actual vs predicted total ticket prices
import matplotlib.pyplot as plt

# Convert Spark DataFrame to Pandas DataFrame for visualization
pandas_comparison = comparison.toPandas()

# Plot actual vs predicted total ticket prices
plt.figure(figsize=(10, 6))
plt.scatter(pandas_comparison["total_ticket_price"], pandas_comparison["predicted_total_ticket_price"], alpha=0.5)
plt.plot([pandas_comparison["total_ticket_price"].min(), pandas_comparison["total_ticket_price"].max()],
         [pandas_comparison["total_ticket_price"].min(), pandas_comparison["total_ticket_price"].max()],
         color='red', linestyle='--')
plt.xlabel("Actual Total Ticket Price")
plt.ylabel("Predicted Total Ticket Price")
plt.title("Actual vs Predicted Total Ticket Price")
plt.show()

from pyspark.sql.functions import sum as _sum

# Calculate the total predicted revenue and total actual revenue
total_predicted_revenue = predicted_total_ticket_price.select(_sum("prediction").alias("total_predicted_revenue")).collect()[0][0]
total_actual_revenue = predicted_total_ticket_price.select(_sum("total_ticket_price").alias("total_actual_revenue")).collect()[0][0]

# Calculate the added revenue
added_revenue = total_predicted_revenue - total_actual_revenue

# Show the results
print(f"Total Predicted Revenue: ${total_predicted_revenue:.2f}")
print(f"Total Actual Revenue: ${total_actual_revenue:.2f}")
print(f"Added Revenue: ${added_revenue:.2f}")

from pyspark.sql.functions import col, sum as _sum

# Calculate the total predicted revenue and total actual revenue
total_predicted_revenue = predicted_total_ticket_price.agg(_sum("prediction").alias("total_predicted_revenue")).collect()[0]["total_predicted_revenue"]
total_actual_revenue = predicted_total_ticket_price.agg(_sum("total_ticket_price").alias("total_actual_revenue")).collect()[0]["total_actual_revenue"]

# Calculate the total number of tickets
total_tickets = predicted_total_ticket_price.count()

# Calculate the average predicted ticket price and average actual ticket price
average_predicted_ticket_price = total_predicted_revenue / total_tickets
average_actual_ticket_price = total_actual_revenue / total_tickets

# Calculate the average increase in revenue per ticket
average_increase_per_ticket = average_predicted_ticket_price - average_actual_ticket_price

# Print the results
print(f"Average Predicted Ticket Price: ${average_predicted_ticket_price:.2f}")
print(f"Average Actual Ticket Price: ${average_actual_ticket_price:.2f}")
print(f"Average Increase in Revenue per Ticket: ${average_increase_per_ticket:.2f}")

"""Problem Statement
Despite ongoing data-driven strategies to optimize ticket pricing and fan engagement, challenges persist in enhancing the soccer fan experience in the US across several dimensions:
• Fan Knowledge and Engagement: Current strategies fall short in accurately identifying and utilizing deep insights from fan profiling, leading to generic marketing efforts and missed opportunities for targeted sponsorships.
• Data Utilization and Monetization: Collecting fan data isn't enough. A systematic approach to integrate and monetize this data through innovative strategies is essential to truly enhance fan experiences and drive revenue.
• Fan Experience Innovation: Current fan experiences need reimagining to leverage data, creating memorable engagements that extend beyond the stadium.
Importance of BMO Stadium and Team Dynamics
• BMO Stadium: A leading venue with a record of over 1.3 million tickets sold in various events, providing invaluable data for understanding fan behaviors and refining engagement strategies.
• Inter Miami CF and Nashville SC: Teams like Inter Miami CF, with high-profile figures like Lionel Messi and David Beckham as its co-owner, and emerging teams like Nashville SC, provide unique insights into fan engagement and market dynamics, critical for tailoring strategies in competitive sports landscapes.
Solution Approach
To address these challenges, we propose a multifaceted strategy focused on data integration, predictive analytics, and personalized fan engagement:
• Data Integration and Cleansing: Amalgamate and refine data from multiple sources like SeatGeek and Ticketmaster to enhance fan profiling.
• Advanced Predictive Analytics: Utilize PySpark to develop dynamic pricing models and machine learning algorithms to tailor marketing strategies.
• Innovative Fan Experience Strategies: Leverage insights to innovate both in-stadium and digital fan experiences.
• Real-Time Dynamic Pricing and Marketing: Adapt pricing and marketing based on fan engagement.
Expected Outcomes
• Increased Revenue: Through enhanced pricing strategies and targeted marketing, we aim to boost ticket sales and overall revenue.
• Enhanced Fan Experience: By offering personalized engagements, we anticipate increased fan satisfaction and loyalty.
• Operational Efficiency: Leverage AI and advanced analytics to streamline operations and enable quick, accurate decision-making.
By tackling these challenges with a robust, data-driven approach that integrates detailed analytics and innovative strategies, this project aims to transform the fan experience and achieve significant gains in revenue management and fan satisfaction.
"""